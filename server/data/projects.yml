---
- _id: ldenigermaster
  type: master
  student: Christophe Deniger
  supervisor: Alphonsine Beauchamps
  cosupervisor: Michel Deslauriers
  current: false
  year: 2016
  title:
    fr: Évaluation et amélioration de la qualité de DBpedia pour la représentation de la connaissance du domaine
    en: Evaluation and enhancement of DBpedia quality for representing domain knowledge
  description:
    fr: |
      "L’évolution récente du Web sémantique, tant par la quantité d’information offerte que par la multiplicité des usages possibles, rend indispensable l’évaluation de la qualité des divers ensembles de données (datasets) disponibles. Le Web sémantique étant basé sur la syntaxe RDF, i.e. des triplets <sujet, relation, objet> (par exemple <Montréal, est une ville de, Québec>), on peut le voir comme un immense graphe, où un triplet relie un nœud « sujet » et un nœud « objet » par une arête « relation ». Chaque dataset représente ainsi un sous-graphe. Dans cette représentation, DBpedia, un des datasets majeurs du Web sémantique, en est souvent considéré comme le nœud central. En effet, DBpedia a pour vocation, à terme, de pouvoir représenter toute l’information présente dans Wikipedia, et couvre donc une très grande variété de sujets, permettant de faire le lien avec tous les autres datasets, incluant les plus spécialisés. C’est de cette multiplicité des sujets couverts qu’apparait un point fondamental de ce projet : la notion de « domaine ». Informellement, nous considérons un domaine comme étant un ensemble de sujets reliés par une thématique commune. Par exemple, le domaine Mathématiques contient plusieurs sujets, comme algèbre, fonction ou addition. Formellement, nous considérons un domaine comme un sous-graphe de DBpedia, où l’on ne conserve que les nœuds représentant des concepts liés à ce domaine.
      En l’état actuel, les méthodes d’extraction de données de DBpedia sont généralement beaucoup moins efficaces lorsque le sujet est abstrait, conceptuel, que lorsqu’il s’agit d’une entité nommée, par exemple une personne, ville ou compagnie. Par conséquent, notre première hypothèse est que l’information disponible sur DBpedia liée à un domaine est souvent pauvre, car nos domaines sont essentiellement constitués de concepts abstraits. La première étape de ce travail de recherche fournit une évaluation de la qualité de l’information conceptuelle d’un ensemble de 17 domaines choisis semi-aléatoirement, et confirme cette hypothèse. Pour cela, nous identifions plusieurs axes permettant de chiffrer la « qualité » d’un domaine : 1 - nombre de liens entrants et sortants pour chaque concept, 2 - nombre de liens reliant deux concepts du domaine par rapport aux liens reliant le domaine au reste de DBpedia, 3 - nombre de concepts typés (i.e. représentant l’instance d’une classe, par exemple Addition est une instance de la classe Opération mathématique : le concept Addition est donc typé si la relation <addition, instance de, opération mathématique> apparait dans DBpedia). Nous arrivons à la conclusion que l’information conceptuelle contenue dans DBpedia est effectivement incomplète, et ce selon les trois axes.
      La seconde partie de ce travail de recherche est de tenter de répondre au problème posé dans la première partie. Pour cela, nous proposons deux approches possibles. La première permet de fournir des classes potentielles, répondant en partie à la problématique de la quantité de concepts typés. La seconde utilise des systèmes d’extraction de relations à partir de texte (ORE – Open Relation Extraction) sur l’ABSTRACT (i.e. premier paragraphe de la page Wikipedia) de chaque concept. En classifiant les relations extraites, cela nous permet 1) de proposer des relations inédites entre concepts d’un domaine, 2) de proposer des classes potentielles, comme dans la première approche. Ces deux approches ne sont, en l’état, qu’un début de solution, mais nos résultats préliminaires sont très encourageants, et indiquent qu’il s’agit sans aucun doute de solutions pertinentes pour aider à corriger les problèmes démontrés dans la première partie."
    en: |
      "In the current state of the semantic web, the quantity of available data and the multiplicity of its uses impose the continuous evaluation of the quality of this data, on the various Linked Open Data (LOD) datasets. These datasets are based on the RDF syntax, i.e. <subject, relation, object> triples, such as <Montréal, is a city of, Québec>. As a consequence, the LOD cloud can be represented as a huge graph, where every triple links the two nodes “subject” and “object”, by an edge “relation”. In this representation, each dataset is a sub-graph. DBpedia, one of the major datasets, is colloquially considered to be the central hub of this cloud. Indeed, the ultimate purpose of DBpedia is to provide all the information present in Wikipedia, “translated” into RDF, and therefore covers a wide range of domains, allowing a linkage with every other LOD dataset, including the most specialized. From this wide coverage arises one of the fundamental concepts of this project: the notion of “domain”. Informally, a domain is a set of subjects with a common thematic. For instance, the domain Mathematics contains several subjects such as algebra, function or addition. More formally, a domain is a sub-graph of DBpedia, where the nodes represent domain-related concepts.
      Currently, the automatic extraction methods for DBpedia are usually far less efficient when the target subject is conceptual than when it is a named entity (such as a person, city or company). Hence our first hypothesis: the domain-related information available on DBpedia is often poor, since domains are constituted of concepts. In the first part of this research project, we confirm this hypothesis by evaluating the quality of domain-related knowledge in DBpedia for 17 domains chosen semi-randomly. This evaluation is based on three numerical aspects of the “quality” of a domain: 1 – number of inbound and outbound links for each concepts, 2 – number of links between two domain concepts compared to the number of links between the domain and the rest of DBpedia, 3- number of typed concepts (i.e. representing the instance of a class : for example, Addition is an instance of the class Mathematical operation : the concept Addition is typed if the relation <addition, type, mathematical operation> appears in DBpedia). We reach the conclusion that the domain-related, conceptual information present in DBpedia is indeed poor on the three axis.
      In the second half of this work, we give two solutions to the quality problem highlighted in the first half. The first one allows to propose potential classes that could be added in DBpedia, addressing the 3rd quality aspect: number of typed concepts. The second one uses an Open Relation Extraction (ORE) system that allows to detect relations in a text. By using this system on the abstract (i.e. the first paragraph of the Wikipedia page) of each concept, and classifying the extracted relation depending on their semantic meaning, we can 1) propose novel relations between domain concepts, and 2) propose additional potential classes. These two methods currently only represent the first step, but the preliminary results we obtain are very encouraging, and seem to indicate that they are absolutely relevant to help correcting the issues highlighted in the first part."
  thesisUrl: "https://publications.polymtl.ca/2409/"
  publications:
    - deniger2017
    - gelinas2016
    - deniger2015

- _id: fferrymaster
  type: master
  student: Granella Lacoste
  supervisor: Michel Deslauriers
  cosupervisor: Alphonsine Beauchamnps
  current: false
  year: 2018
  title:
    fr: "Extraction de relations sémantiques à partir de descriptions de sites patrimomiaux du Québec"
    en: "Extraction of semantic relations from descriptions of Quebec heritage sites"
  description:
    fr: |
      Une grande partie de l’information présente sur le web et dans les bases de données l’est sous forme de textes. Ces données sont difficilement exploitables de façon automatique et il est impossible de procéder à des requêtes particulières sur celles-ci, puisqu’elles ne sont pas décrites par des métadonnées. Structurer ces données est un enjeu de taille qui permettra de les rendre plus accessibles et exploitables. De nombreuses méthodes d’extraction d’informations à partir de textes bruts ont vu le jour. Les plus répandues reposent sur des algorithmes d’apprentissage automatique et font appel à différentes techniques pour représenter les mots. Ces techniques sont indispensables et permettent de mettre en valeur certaines informations, comme la nature des mots, leur fonction, leur répartition dans le corpus, ou encore leur sémantique.
      Dans le cadre de ce projet, nous allons travailler avec les données du Répertoire du Patrimoine Culturel du Québec. Ce répertoire inventorie l’ensemble du patrimoine immobilier, mobilier et immatériel du Québec. Toutefois, la classification actuelle présente des problèmes majeurs et ne répond plus aux besoins du Ministère de la Culture et des Communications du Québec (MCC). C’est pourquoi, en vue d’une refonte de la base de connaissances, le MCC nous a proposé de nous intéresser aux relations pouvant exister entre des biens immobiliers et des personnes (physiques ou morales). Ces relations sont décrites dans les synthèses historiques des biens immobiliers ; des textes décrivant chacun l’histoire d’un bien immobilier. Il existe déjà des relations modélisées dans le répertoire, mais dans l’optique d’une refonte de la classification, nous proposons une application capable de peupler de façon automatique la future base de connaissances. Les données d’entrée de notre problème sont donc, pour chaque bien immobilier, une synthèse historique relatant l’histoire du bien immobilier et une liste de personnes qui ont été en relation avec ledit bien.
      La question de recherche est de savoir si une approche basée sur l’apprentissage machine est suffisante pour extraire les relations à partir de ces synthèses.
    en: |
      A lot of information on the Web and in databases is in raw texts. If the raw text is easily understandable for humans, it is more difficult to process it with machines. This is why structuring data is a big challenge, that will allow making data more accessible and exploitable. There are numerous information extraction methods from raw texts. The most popular are based on machine learning and word representation to take into account some information like semantic, word distribution, etc.
      In this project, we will work with data from the Repertoire of Cultural Heritage of Quebec. This repertoire brings together real estates, person, movable heritage and intangible cultural heritage of Quebec. The current classification does no longer meet the needs of the Ministry of Culture and Communication of Quebec. This is why, to help to redesign the knowledge base, we propose an application to extract relations between real estates and persons or group of persons. Each real estate has a historical synthesis which describes its history, and cite persons who played some role in its history. Thus, our goal is to process these syntheses to extract these relations. Ultimately, this application should help to settle the future knowledge base. Input data of our problem are, for each real estate, a historical synthesis and a list of persons who are in relation with this real estate.
      Our research question is to determine if a machine learning-based approach is enough to extract relations from the syntheses. For each pair hreal estate, personi, we will first isolate the context around each mention of the person in the historical synthesis of the real estate. We found out, by browsing the data, that information describing relation is very often near the mention of the person. We define the context either by a fixed number of words surrounding the mention, either by the sentence containing the mention. Then we use a word representation model to transform context into a vector. Thus, we have a vector for each pair hreal estate, personi. This vector will then be given to a supervised machine learning algorithm (support vector machine or multilayer perceptron) to predict the relation it represents. These algorithms are trained on data extracted from the Repertoire of Cultural Heritage of Quebec, and are tested on a manually annotated corpus (extracted from the repertoire and annotated by us).
  thesisUrl: "https://publications.polymtl.ca/3301/"

- _id: scorriveaumaster
  type: master
  student: "Gustina Jutras"
  supervisor: Philippe Bond
  cosupervisor: Michel Deslauriers
  current: false
  title:
    fr: "Utilisation des technologies du Web sémantique pour la sécurité d'information"
    en: "Semantic web technologies for computer security"

- _id: pandersonmaster
  type: master
  student: Patrick Anderson
  supervisor: Alphonsine Beauchamnps
  cosupervisor: Michel Deslauriers
  current: false
  year: 2016
  title:
    fr: "Amélioration des performances des annotateurs sémantiques"
    en: "Improvement of the performance of semantic annotators"
  description:
    fr: 'Les annotateurs sémantiques jouent un rôle important dans la transition du Web actuel au Web sémantique. Ils s’occupent d’extraire des informations structurées à partir de textes bruts, permettant ainsi de pointer vers des bases des connaissances telles que DBpedia, YAGO ou Babelnet. De nombreuses compétitions sont organisées chaque année pour promouvoir les travaux de recherche de ce domaine. Nous présentons dans ce mémoire notre participation à la compétition Open Knowledge Extraction que nous avons remportée à la conférence European Semantic Web Conference 2016. Dans le cadre de cette compétition, nous avons implémenté une approche générique que nous avons testée sur quatre annotateurs sémantiques. Nous nous concentrons dans ce mémoire à décrire un annotateur sémantique en particulier, DBpedia Spotlight. Nous exposons les différentes limites que présente cet annotateur ainsi que les approches que nous avons développées pour y remédier. Nous avons noté une augmentation d’une moyenne de 20% des performances actuelles de DBpedia Spotlight en testant sur différents corpus. Ces derniers proviennent principalement de journaux internationaux, "Reuters News Stories", "MSNBC" et le "New York Times".'
    en: 'Semantic annotators play an important role in the transition from the current Web to the Semantic Web. They take care of extracting structured information from raw texts and thus make it possible to point to knowledge bases such as DBpedia, YAGO or Babelnet. Many competitions are organized every year to promote research works in this field. We present in this thesis our system which was the winner of the Open Knowledge Extraction challenge at the European Semantic Web Conference 2016. For this competition, we implemented a generic approach tested with four semantic annotators. We particularly focus in this thesis on one semantic annotator, DBpedia Spotlight. We present its different limitations along with the approaches that we have developed to remedy them. We noted an improvement of an average of 20% of the current performance of DBpedia Spotlight on different corpora that mainly come from international newspapers, "Reuters News Stories", "MSNBC" and the "New York Times".'
  thesisUrl: https://publications.polymtl.ca/2373/
  publications:
    - anderson2018

- _id: jgelinasmaster
  type: master
  student: Joey Gelinas
  supervisor: Alphonsine Beauchamnps
  cosupervisor: Michel Deslauriers
  current: false
  year: 2017
  title:
    fr: "Extraction d'axiomes et de règles logique à partir de définitions de Wikipédia en langage naturel"
    en: "Extraction of axioms and logical rule from Wikipeda definitions"
  description:
    fr: "Le Web sémantique repose sur la création de bases de connaissances complexes reliant les données du Web. Notamment, la base de connaissance DBpedia a été créée et est considérée aujourd’hui comme le « noyau du réseau Linked Open Data ». Cependant DBpedia repose sur une ontologie très peu riche en définitions de concepts et ne prend pas en compte l’information textuelle de Wikipedia. L’ontologie de DBpedia contient principalement des liens taxonomiques et des informations sur les instances. L’objectif de notre recherche est d’interpréter le texte en langue naturelle de Wikipédia, afin d’enrichir DBpedia avec des définitions de classes, une hiérarchie de classes (relations taxonomiques) plus riche et de nouvelles informations sur les instances. Pour ce faire, nous avons recours à une approche basée sur des patrons syntaxiques implémentés sous forme de requêtes SPARQL. Ces patrons sont exécutés sur des graphes RDF représentant l’analyse syntaxique des définitions textuelles extraites de Wikipédia. Ce travail a résulté en la création de AXIOpedia, une base de connaissances expressive contenant des axiomes complexes définissant les classes, et des triplets rdf:type reliant les instances à leurs classes."
    en: "The Semantic Web relies on the creation of rich knowledge bases which links data on the Web. In that matter, DBpedia started as a community effort and is considered today as the central interlinking hub for the emerging Web of data. However, DBpedia relies on a lighweight ontology and deals with some substantial limitations and lacks some important information that could be found in the text and the unstructured data of Wikipedia. Furthermore, the DBpedia ontology contains mainly taxonomical links and data about the instances, and lacks class definitions. The objective of this work is to enrich DBpedia with class definitions and taxonomical links using text in natural language. For this purpose, we rely on a pattern-based approach that transforms textual definitions from Wikipedia into RDF graphs, which are processed to query syntactical pattern occurrences using SPARQL. This work resulted in the creation of AXIOpedia, a rich knowledge base containing complex axioms defining classes and rdf:type relations relating instances with these classes."
  thesisUrl: https://publications.polymtl.ca/2566/
  publications:
    - gelinas2016-2
    - gelinas2016

- _id: klangemaster
  type: master
  student: Simoneau Fleurilius
  supervisor: Michel Deslauriers
  cosupervisor: Alphonsine Beauchamnps, Jean-Marc Parent
  current: false
  year: 2016
  title:
    fr: "Amélioration de la précision de systèmes d'extraction de relations en utilisant un filtre générique basé sur l'apprentissage statistique"
    en: "Improvement of the precision of relation extraction system using a machine learning-based filter"
  description:
    fr: "L’extraction de relations contribue à l’amélioration de la recherche sémantique, recherche basée sur la compréhension du sens des termes de recherche. Puisque la recherche d’information est principalement axée sur des mots-clés, l’extraction de relations offre un éventail de possibilités en identifiant les liens entre les entités. L’extraction de relations permet entre autres de transformer de l’information non structurée en information structurée. Les bases de connaissances,telles que Google Knowledge Graph et DBpedia, permettent un accès plus précis et plus direct à l’information. Le slot filling, qui consiste à peupler une base de connaissances à partir de textes, a été une tâche très active depuis quelques années faisant l’objet de plusieurs campagnes évaluant la capacité d’extraire automatiquement des relations prédéfinies d’un corpus de documents. Malgré quelques progrès, les résultats de ces compétitions demeurent modestes. Nous nous concentrons sur la tâche de slot filling dans le cadre de la campagne d’évaluation TAC KBP 2013. Cette tâche vise l’extraction de 41 relations prédéfinies basées sur les infobox de Wikipédia (par exemple: title, date of birth, countries of residence, etc.)liées à des entités nommées spécifiques (personnes et organisations). Une entité nommée (l’entité requête) et une relation sont soumises à un système (extracteur de relations) qui doit automatiquement trouver, parmi un corpus de plus de deux millions de documents, toute entité liée à l’entité requête par la relation donnée. Le système doit également retourner un segment textuel justifiant cette relation. Ce mémoire présente un filtre basé sur l’apprentissage statistique dont l’objectif principal est d’améliorer la précision d’extracteurs de relations tout en minimisant l’impact sur le rappel. Notre approche consiste à filtrer la sortie des extracteurs de relations en utilisant un classifieur. Notre filtre est annexé à la sortie de l’extracteur de relations, pouvant ainsi être facilement testé sur n’importe quel système. Notre classifieur est basé sur un large éventail de caractéristiques (features), incluant des caractéristiques statistiques, lexicales, morphosyntaxiques, syntaxiques et sémantiques extraites en majorité des phrases justificatives soumises par les systèmes. Nous proposons également une méthode efficace permettant d’extraire les patrons les plus fréquents (ex.: catégories orphosyntaxiques, dépendances syntaxiques) afin d’en dériver des caractéristiques booléennes utiles pour notre tâche de filtrage. Les caractéristiques utilisées pour l’entraînement des classifieurs sont soit génériques. Ainsi, notre méthode peut être utilisée pour la classification de toute relation prédéfinie. Nous avons testé le filtre sur 14 systèmes ayant participé à la tâche de slot filling. Le filtre permet d’améliorer la précision pour chacun de ces systèmes. Nos résultats démontrent également que le filtre permet d’améliorer la précision du meilleur système de plus de 20% (points de pourcentage) et d’améliorer le F-score pour 20 relations."
    en: "In the current state of the semantic web, the quantity of available data and the multiplicity of its uses impose the continuous evaluation of the quality of this data, on the various Linked Open Data (LOD) datasets. These datasets are based on the RDF syntax, i.e. <subject, relation, object> triples, such as <Montréal, is a city of, Québec>. As a consequence, the LOD cloud can be represented as a huge graph, where every triple links the two nodes “subject” and “object”, by an edge “relation”. In this representation, each dataset is a sub-graph. DBpedia, one of the major datasets, is colloquially considered to be the central hub of this cloud. Indeed, the ultimate purpose of DBpedia is to provide all the information present in Wikipedia, “translated” into RDF, and therefore covers a wide range of domains, allowing a linkage with every other LOD dataset, including the most specialized. From this wide coverage arises one of the fundamental concepts of this project: the notion of “domain”. Informally, a domain is a set of subjects with a common thematic. For instance, the domain Mathematics contains several subjects such as algebra, function or addition. More formally, a domain is a sub-graph of DBpedia, where the nodes represent domain-related concepts. Currently, the automatic extraction methods for DBpedia are usually far less efficient when the target subject is conceptual than when it is a named entity (such as a person, city or company). Hence our first hypothesis: the domain-related information available on DBpedia is often poor, since domains are constituted of concepts. In the first part of this research project, we confirm this hypothesis by evaluating the quality of domain-related knowledge in DBpedia for 17 domains chosen semi-randomly. This evaluation is based on three numerical aspects of the “quality” of a domain: 1 – number of inbound and outbound links for each concepts, 2 – number of links between two domain concepts compared to the number of links between the domain and the rest of DBpedia, 3- number of typed concepts (i.e. representing the instance of a class : for example, Addition is an instance of the class Mathematical operation : the concept Addition is typed if the relation <addition, type, mathematical operation> appears in DBpedia). We reach the conclusion that the domain-related, conceptual information present in DBpedia is indeed poor on the three axis. In the second half of this work, we give two solutions to the quality problem highlighted in the first half. The first one allows to propose potential classes that could be added in DBpedia, addressing the 3rd quality aspect: number of typed concepts. The second one uses an Open Relation Extraction (ORE) system that allows to detect relations in a text. By using this system on the abstract (i.e. the first paragraph of the Wikipedia page) of each concept, and classifying the extracted relation depending on their semantic meaning, we can 1) propose novel relations between domain concepts, and 2) propose additional potential classes. These two methods currently only represent the first step, but the preliminary results we obtain are very encouraging, and seem to indicate that they are absolutely relevant to help correcting the issues highlighted in the first part."
  thesisUrl: https://publications.polymtl.ca/2115/
  publications:
    - fleurilius2018
    - fleurilius2016

- _id: mtrudelphd
  type: phd
  student: Hildège Trudel
  supervisor: Michel Deslauriers
  cosupervisor: Rachid Badouri
  current: false
  year: 2012
  title:
    fr: "Système d'aide à la décision pour le réseau de distribution"
    en: "Decision-support support for electrical distribution network"
  description:
    fr: "De nos jours, de nouvelles technologies issues du domaine de l'information et de la communication sont introduites progressivement dans les réseaux de distribution électrique. Ces technologies nécessitent des études poussées et des simulations précises afin d'en évaluer les forces et les faiblesses. Toutefois, la simulation des réseaux électriques demeure une tâche complexe qui nécessite de tenir compte de plusieurs facteurs : électriques, mécaniques, économiques, naturels, matériels et humains. Pour pallier à la complexité inhérente à la simulation électrique, il est possible de recourir aux systèmes multiagents (SMA). Ils présentent de nombreux avantages. Ils offrent une grande flexibilité en permettant à des agents autonomes de collaborer pour atteindre des objectifs complexes. Le SMA, par opposition au système de simulation monolithique, présente l'avantage d'être une architecture souple et évolutive capable de traiter des opérations complexes. Toutefois, le développement et la manipulation de ces systèmes sont des tâches réservées à des experts en informatique et en SMA. Or, dans le cadre du projet LEOPAR, mené à l'Institut de recherche d'Hydro Québec, nous avons comme principal objectif de développer un SMA accessible à des non-experts en informatique. Le but est de permettre aux décideurs et aux ingénieurs électriques de modifier et de faire évoluer le simulateur de la manière la plus aisée possible. Pour ce faire, nous avons développée une architecture à mi-chemin entre les architectures de Tableau Noir et les SMA. Nous avons utilisé une zone distribuée de partage de données pour permettre la communication des agents. Le partage et l'échange d'informations se fait par la modification des données distribuées. Ce mécanisme réduit la complexité des agents et leur mode de communication. De plus, nous avons spécifié un langage d'actions de haut niveau qui permet de décrire de manière déclarative les actions, leurs effets, leurs conditions et leurs relations. Ce langage d'actions est automatiquement traduit en logique non monotone (Answer Set Programming) afin de permettre la coordination des agents du simulateur. La traduction que nous proposons du langage d'actions surpasse largement les autres langages d'actions en termes de rapidité d'exécution lors de la planification. La combinaison de notre langage d'actions et de la logique non monotone a permis le développement d'un système performant, qui offre la possibilité à des novices de rajouter, modifier ou supprimer des agents du simulateur. Le simulateur multiagents que nous avons développé fonctionne adéquatement et permet, entre autre, de réaliser des simulations de type Monte-Carlo pour l’étude de la fiabilité des réseaux. Notre simulateur permet de quantifier, à l'aide des indices de performances, l'impact et l'apport de nouvelles technologies. Il est en mesure de reproduire avec une grande fidélité des phénomènes électriques, mécaniques et humains, tels que la surcharge électrique des câbles, le changeur de prise des transformateurs, les équipes humaines d'intervention, le temps de restauration variable et la reconfiguration du réseau. Notre simulateur a été testé sur de véritables réseaux de distribution d'Hydro-Québec et a démontré sa capacité à traiter de grandes quantités de données. En comparaison à d'autres simulateurs électriques multiagents standards, notre système s'est avéré être tout aussi performant mais beaucoup plus facile à développer et à faire évoluer. Lors des simulations électriques, nous avons été en mesure de réaliser des études de fiabilité qui ont permis de déterminer les facteurs les plus importants influant les performances du réseau."
    en: "Nowadays, the information system technologies are increasingly used in power distribution systems to improve network reliability and performance. The impact of these structural changes is important and requires in-depth studies and investigations. A better understanding of the effect of these technologies is required to optimize the network. However, the simulation of power network is a complex task, where several technical issues need to be considered such as : electrical, mechanical, economical, natural and human aspects. The idea is to develop a multi-agent system (MAS) that can process complex simulations. Such a system is extensible and modular and it is composed by numerous simple agents that can collaborate and interact in order to achieve complex objectives. Multi-agent systems are capable of reaching goals that are difficult to achieve by monolithic systems or individual agents, which can be complex and hard to maintain and extend. Nevertheless, the development and the maintenance of a MAS is a complex task that has to be performed by experts on computer science and multi-agent systems. In the framework of the project LEOPAR, carried out by the \textit{Institute de Recherche d'Hydro-Québec}, we have as a main objective to develop an accessible and comprehensive MAS. The project's aim was to allow managers to modify the behavior and the objectives of the simulator without the assistance of an expert. To this end, we developed a simulator based on Blackboard and MAS. Our system relies on a common pool of data to share information between agents. This type of mechanism reduces the communication complexity and makes the development of agents easier. In addition, we defined a new action language that allows to incrementally describe the agent's actions, effects, conditions and relations. Our action language is automatically translated into a non-monotonic logic (Answer Set Programming) in order to process the agent's actions. The translated answer set program has shown to be effective in providing action plans. The action language combined to answer set programming allowed us to develop a powerful and accessible simulator, enabling novice to add, change, and remove agents' behavior. Our simulator works properly and allows, among other things, processing power network assessments using a Monte-Carlo approach. It analyses the impact of introducing new types of technologies, by comparing performance indicators of the network. Moreover, it is able to simulate with accuracy a wide variety of phenomena as wire overloading, protection mechanism activation, tap changer changes, human intervening team patrols, restoration process and network reconfiguration. It has been tested on realistic distribution network of Hydro-Quebec and it performed well in assessing networks. Our simulator is performing similarly to a classical multi-agents system, but with the benefit of being accessible and easy to use."
  thesisUrl: https://publications.polymtl.ca/1040
  publications:
    - trudel2011

- _id: nstlouisphd
  type: phd
  student: Xavier St-Louis
  supervisor: Michel Deslauriers
  current: false
  year: 2016
  title:
    fr: "QED-Tutrix : système tutoriel intelligent pour l'accompagnement des élèves en situation de résolution de problèmes de démonstration en géométrie plane"
    en: "QED-Tutrix: intelligent tutorial system for student support in problem solving demonstration in flat geometry"
  description:
    fr: "Au cours des dernières années, le système scolaire québécois impose une pression croissante sur les enseignants. En effet, ceux-ci doivent gérer des classes de plus en plus nombreuses tout en maintenant un soutien adéquat à l’apprentissage des élèves. Dans ce contexte, l’utilisation de systèmes tutoriels intelligents qui sont en mesure d’assister l’enseignant dans son travail pourrait permettre à ce dernier de consacrer plus de temps aux élèves qui en ont véritablement besoin. Malheureusement, dans le domaine de l’enseignement des preuves en géométrie, l’offre de systèmes tutoriels est limitée. De plus, ceux actuellement proposés forcent l’élève à travailler selon un ordre déterminé et ils ne fournissent pas de soutien dans le cadre d’une exploration libre du problème. Ils l’obligent aussi à rédiger des preuves formelles qui ne sont pas adaptées aux exigences des enseignants du secondaire. Partant de ce constat, nous avons établi l’objectif principal de notre projet qui consiste à proposer un système tutoriel intelligent qui assiste l’élève dans une démarche d’exploration plutôt que de rédaction dans le cadre de l’élaboration d’une preuve en géométrie.\nDans le but de l’atteindre, nous proposons le système QED-Tutrix qui a été conçu à la suite d’une analyse des interventions d’enseignants réels observés. Il permet à l’enseignant ou au didacticien de construire un ensemble de preuves acceptables pour un problème donné en fonction de l’objectif d’apprentissage visé. L’élève peut ensuite tenter de résoudre le problème choisi par l’enseignant en utilisant les différents outils offerts à l’interface de QED-Tutrix. Celui-ci a accès à une figure dynamique afin de découvrir des conjectures, à un répertoire d’énoncés pour composer sa preuve et à une démonstration qu’il doit compléter. Tous les énoncés proposés sont analysés par le système qui génère des rétroactions à l’intérieur d’une fenêtre de clavardage afin de guider l’élève lors de l’exploration et de la résolution du problème. L’élaboration de QED-Tutrix a été réalisée par une équipe multidisciplinaire composée d’experts en didactique et en informatique. Le système a été construit itérativement par la mise en oeuvre du paradigme de la conception dans l’usage qui est constitué d’une succession de plusieurs cycles de recherche et de développement. Chaque cycle se clôture par une expé- rimentation qui vise à valider le travail accompli et à recueillir des informations qui sont réinvesties dans le cycle suivant. Une première version du système (GeoGebraTUTOR) a donc été créée afin d’étudier notamment les interventions d’enseignants réels qui ont inspiré l’élaboration de la seconde version (QED-Tutrix), qui est décrite et analysée dans cette thèse. Nous ne prétendons pas que la version actuelle du système a un effet mesurable sur les résultats scolaires, car nous visons, pour l’instant, à permettre à un élève de travailler en conformité avec des théories didactiques reconnues. En effet, la conception de QED-Tutrix s’ancre principalement dans la théorie des situations didactiques qui permet de représenter une situation didactique par une relation élève-milieu. Nous utilisons cependant une version étendue de cette théorie dans laquelle un agent tutoriel, qui joue le rôle d’un enseignant virtuel, peut agir sur la relation élève-milieu. De plus, nous désirons offrir un système tutoriel qui est un véritable espace de travail géométrique, c’est-à-dire qu’il permet à l’élève de résoudre des problèmes en mettant en oeuvre les trois démarches définies dans cet espace. Ces théories didactiques ainsi que les résultats de nos observations ont été implantés dans QED-Tutrix. Il en est résulté un système comportant quatre couches logicielles principales. La première permet de modéliser l’ensemble des démonstrations possibles pour résoudre un problème donné. Pour chaque problème, l’enseignant inscrit toutes les inférences, ou pas de démonstration, qui sont acceptables pour sa résolution selon l’objectif d’apprentissage visé. Chaque inférence contient une justification qui est utilisée pour produire un conséquent à partir de l’ensemble de ses antécédents. Il est possible de les combiner afin d’obtenir un graphe contenant toutes les solutions valides, car les conséquents peuvent être recyclés pour former les antécédents d’autres inférences. Le parcours du graphe contenu dans cette première couche permet donc d’énumérer les différentes solutions au problème représenté. Afin de proposer une aide qui respecte l’état cognitif de l’élève lors de l’exploration d’un problème, il est essentiel de conserver la chronologie de ses actions. Nous l’avons donc modélisée à l’intérieur de la deuxième couche de notre système. Celle-ci contient des données dynamiques qui sont mises à jour au cours de la résolution d’un problème, à l’opposé du graphe qui est statique, et elle se superpose à ce dernier. En effet, nous indiquons, pour chaque noeud du graphe, le temps d’activation le plus récent qui correspond à l’écriture de l’énoncé qui lui est attaché. Cette approche se démarque de celle des autres systèmes, car ces derniers n’utilisent pas la chronologie des actions, étant donné qu’ils imposent une séquence de résolution. Pour être en mesure de suggérer différentes pistes de solution à un élève bloqué dans son processus de résolution, nous avons choisi de traiter les inférences selon un ordre de priorité. Ce classement est réalisé par la troisième couche de QED-Tutrix, qui utilise les données des deux couches précédentes. Pour l’obtenir, nous recherchons d’abord la solution la plus avancée à l’aide d’une heuristique originale, que nous avons élaborée et qui permet d’éviter d’énumérer toutes les solutions. Nous affectons ensuite des priorités plus élevées aux inférences faisant partie de la solution déterminée et qui ont été travaillées récemment par l’élève, afin de respecter son état cognitif. La proposition d’autres pistes nous démarque des systèmes tutoriels traditionnels qui offrent de l’aide uniquement pour compléter une solution optimale. La liste ordonnée d’inférences est utilisée par la dernière couche du système, soit celle qui produit les différentes rétroactions. Premièrement, QED-Tutrix offre des rétroactions instantanées, en réponse à l’écriture de chaque énoncé, sous forme d’émoticônes et de messages courts. Il permet aussi d’encoder des erreurs courantes afin de leur associer des messages précis. Il offre enfin une aide à la prochaine étape qui est inspirée des interventions des enseignants réels observés. Cette dernière forme d’aide a été modélisée par une machine à états finis qui traite séquentiellement les inférences ordonnées dans la liste et produit une série d’indices permettant de les compléter. Des messages doivent être composés pour cha- cune des inférences, mais des mécanismes ont été implantés afin de réduire leur nombre. Les rétroactions offertes sont comparables à celles d’autres systèmes tutoriels. Environ 450 inférences ont été produites et près de 900 messages composés afin d’implanter les cinq problèmes actuellement offerts dans notre système. Son fonctionnement a d’abord été vérifié par un expert indépendant. Celui-ci a confirmé que les messages produits étaient conformes à la structure déterminée, mais que l’évaluation de la solution la plus avancée était parfois problématique. QED-Tutrix a ensuite été utilisé par des élèves de 4e secondaire. Ceux-ci ont généralement trouvé le système utile et ont apprécié l’expérience. L’analyse des enregistrements nous a permis de constater que la structure des messages générée permet d’ai- der certains élèves. De plus, nous avons observé la mise en oeuvre des différentes démarches, ce qui confirme le statut d’espace de travail géométrique de QED-Tutrix. L’efficacité de ce dernier est, par contre, limitée dans le cas d’élèves plus faibles, car la structure des messages est calibrée afin d’aider des élèves moyens. Le problème concernant l’évaluation de la solution la plus avancée a aussi provoqué la production de messages incohérents avec la stratégie de l’élève. Dans le but d’augmenter l’efficacité du système, nous envisageons, entre autres, de proposer des profils de tuteurs et d’élèves. Malgré les lacunes qui ont été détectées, il n’en demeure pas moins que QED-Tutrix est un système tutoriel innovateur. En effet, dans le domaine des preuves en géométrie, il est le seul à utiliser des émoticônes et à proposer différentes pistes de solution. De plus, son élaboration itérative, par une équipe multidisciplinaire, permet d’obtenir un système respectueux du travail de l’élève, ce qui se démarque de l’approche traditionnelle qui consiste à tenter de reproduire le raisonnement d’un expert. Les étapes suivantes de conception visent à intégrer des rétroactions sous forme de problèmes connexes et à proposer une aide à la construction de la figure. Notre système pourrait facilement être adapté au traitement des démonstrations en logique de premier ordre. Une adaptation pour le traitement du raisonnement sous forme d’argumentation non formelle pourrait aussi être envisagée. Enfin, la suggestion de diverses pistes de solution pourrait être implantée dans d’autres systèmes tutoriels."
    en: "In the past years, Quebec’s school system imposes a growing amount of pressure on its teaching staff. They must juggle classes with more and more students while assuring the quality of their teaching to each of them. In this context, the use of intelligent tutoring systems which can assist the teacher in his or her work could allow the teacher to dedicate more energy to each student when it’s needed. However, the offer for tutoring systems for the learning of proof is limited. Moreover, the available systems force the student to work according to a determined order and they don’t provide help in the context of a free exploration of the problem. They also force the student to write formal proofs when high school teachers rarely demand. With this assessment in mind, we established the principal objective for our project which aims at offering an intelligent tutoring system that assists the student in an exploratory approach when solving geometry proofs instead of a formal proof writing approach.\nThe system we offer is QED-Tutrix which was designed taking into account actual teacher interventions observed in a classroom environment. QED-Tutrix allows the teacher or didactician to construct a number of admissible proofs for a given problem according to the learning goals. The student can then try to solve the problem chosen by the teacher by using the different tools QED-Tutrix puts at his disposal. The student has access to a dynamic geometric figure he can work with in order to make conjectures, as well as to a repertoire of statements to create their proof and an interactive written proof they can use to complete their proof. All the statements he provides are analyzed by the system which then generates feedback through a chat window in order to guide the student during the exploration and solving of the problem. QED-Tutrix’s elaboration was carried out by a multidisciplinary team comprised of experts in didactics of mathematics and in computer science. The system is built in an iterative manner by adopting a design in use approach which consists of a series of many cycles of research and development. Each cycle is ended with an experimentation which aims at validating the work accomplished and at collecting information to be reinvested in the following cycle. A first version of the system (GeoGebraTUTOR) was created to study, among other things, real teacher interventions which inspired the implementation of the second version (QED-Tutrix). This last version is described and analyzed in the following thesis. We do not claim that the present QED-Tutrix version has measurable effects on academic results, since our aim at the moment is to make sure it allows the student to work in a fashion put forward by known didactic principles. Indeed, QED-Tutrix’s conception is rooted mainly in the didactical situation’s theory which represents a didactical context by a student-milieu interaction. However, we use an extended version of this theory in which a tutorial system playing a virtual teaching role may influence this student-milieu interaction. Moreover, we aim at offering a tutoring system that is a true geometrical workspace, meaning that it allows the student to solve problems by engaging in three mathematical processes described in the geometrical workspace model. The didactic theories and conclusions drawn from our observations were implemented in QED-Tutrix. This resulted in a system made of four main software layers. The first of these layers is for the organization of the different proof solutions for a given problem. For each solution, the teacher or didactician registers all the inferences or proof steps which are admissible according to a specific learning context. Each inference includes a justification that is used to produce a result stemming from a group of premises. It is possible to combine the different inferences in order to generate a graph of all the different admissible solutions, since the results of one inference can be recycled as a premise for another. The different pathways of this graph which is the output of the first software layer allow for the account of the different solutions to each implemented problem. In order to offer help that takes into account the cognitive state of the student exploring the problem, it is essential to keep track of the chronology of his or her actions. This memory was implemented in the second layer of the system. It contains information that is dynamic and evolves as problem solving occurs. It also overlays the solution graph which is static. Therefore, we indicate for each of the graphs nodes, the most recent activation time associated with the writing of the statement attached to it. This approach stands out from the way other systems operate and in which chronology is usually not taken into account since a solving sequence is imposed. In order to be able to suggest alternative paths to a student who is stuck in his or her solving process, we chose to treat inferences according to an order of priority. This ranking is carried out by QED-Tutrix’s third layer which uses the data from the first two layers. In order to achieve this ordering, we look for the most advanced solution with the help of an original heuristic, elaborated for this project, which spares the system from running through all the admissible solutions. We then assign the highest priorities to the inferences which are part of the identified solution and that has been worked on recently by the student keeping his or her cognitive state in mind. The ability to suggest other solution option distances us from traditional tutorial systems which offer help only to complete an optimal solution. This list of ranked inferences is used by the fourth and last layer of the system, meaning the layer which generates various feedback. Firstly, QED-Tutrix replies to the writing of each statement with instant feedback in the shape of emojis or short messages. It also allows the programming of particular messages associated with known common mistakes. Lastly, this layer of the system offers help with the next step which is inspired by actual teacher interventions. This last form of help was modelled by a finite state machine that sequentially treats the ranked inferences from the list and produces a series of hints to help the student complete them. Messages must be created for each of the inferences, but mechanisms are implemented in order to reuse messages according to the inferences content, limiting the number of entries. The feedback offered by these messages is similar in form to the feedback offered by other tutorial systems. Approximately 450 inferences were produced and close to 900 messages were created in order to implement the five problems currently available in QED-Tutrix. It’s operating has been verified by an independent expert, which confirmed that the output of messages is true to the identified structure, but the evaluation of the most advanced solution is sometimes problematic. QED-Tutrix was then put through a second trial in a class of 4th year of high school. The students generally found the system to be useful and appreciated the experience. The analysis of the session recordings revealed that the generated messages help some students. Also, we observed different mathematical processes which confirmed QED- Tutrix’s geometrical workspace status. However, the efficiency of QED-Tutrix is limited when helping students with less mathematical abilities since the message structure is built with the average student as a reference. Problems with the identification of the most advanced solution also lead to incoherence between messages and student strategies. In order to enhance the system’s efficiency in helping the student solve problems, we contemplate, among other things, to differentiate tutorial profiles according to students solving profiles. In spite of witnessing shortcomings, QED-Tutrix is an innovative tutorial system. Indeed, in the field of geometry proofs, it is the only automated tutoring system to use emojis and to suggest alternative solution paths. Moreover, the iterative and multidisciplinary approach adopted for its design and development stands out from a traditional approach which aims at reproducing expert reasoning. The next design steps aims at including feedback in the form of related problems and to provide help with building the geometrical figure. Our system could easily be adapted to handle first order logical proofs. An adjustment to process non- formal argumentation could also be considered. Finally, the suggestion of alternative solution paths could be implemented in other tutorial systems."
  thesisUrl: https://publications.polymtl.ca/2450

- _id: ccoulombephd
  type: phd
  student: Zack Garand
  supervisor: Michel Deslauriers
  cosupervisor: Zinedine Zidane, Bernard Geoffrion
  current: true
  title:
    fr: "Apprentissage machine pour la classification de réponses à des questions ouvertes"
    en: "Machine learning for the classification of short answers"

- _id: nbourgoinmaster
  type: master
  student: Leonard Bourgoin
  supervisor: Alphonsine Beauchamnps
  cosupervisor: Michel Deslauriers
  current: true
  title:
    fr: "Système de questions-réponses pour le Web Sémantique"
    en: "Question answering system for the Semantic Web"
  publications:
    - bourgoin2018
    - bourgoin2017a
    - bourgoin2017b

- _id: corbeilmaster
  type: master
  student: Robert De Chateaub
  supervisor: Michel Deslauriers
  cosupervisor: Alphonsine Beauchamnps
  current: false
  year: 2018
  title:
    fr: "Forage de données pour la détection d'un état de blocage de l'apprenant dans le cadre du système tutoriel intelligent QED-Tutrix"
    en: "Data analysis for the detection of a learner blocking state as part of the QED-Tutrix intelligent tutorial system"
  description:
    fr: "L’état de blocage est le moment où un apprenant, en pleine résolution de problème sur un système tutoriel intelligent, a besoin d’une intervention tutorielle pour poursuivre sa résolution. Dans ce mémoire, des modèles probabilistes seront développés pour détecter les états de blocage d’un apprenant qui résout un problème sur le système tutoriel intelligent en mathématiques QED-Tutrix. La méthodologie inclut deux expérimentations avec une version modifiée de QED-Tutrix pour recueillir des séquences d’actions associées à un état de blocage ou de non-blocage. Dans ces ensembles de données, des états de blocage ont été observés à partir des fréquences d’actions et des distributions de sous-séquences. Quatre modèles probabilistes ont été développés en tout : le modèle de processus de fréquence d’actions, le modèle bayésien en sous-séquences d’actions, le modèle du réseau de neurones convolutif et le modèle hybride. Ce dernier surpasse les autres avec un score F1 de 80,4 % pour la classification des états de blocage sur l’ensemble d’entraînement et 77,3 % sur l’ensemble test. L’application de cette recherche mène directement à l’amélioration de la machine à états de QED-Tutrix dans son interaction avec l’apprenant. Elle aboutit aussi sur une deuxième phase de travaux de recherche durant laquelle le développement d’interventions tutorielles ciblées est approché. Puisqu’il est possible d’identifier les moments de blocage de l’apprenant avec une bonne précision, il faut à présent concevoir des algorithmes pouvant comprendre le contexte du blocage et pouvant intervenir en conséquence. En ce qui concerne l’amélioration des performances des modèles, l’incorporation de l’historique des blocages dans les modèles probabilistes est à considérer en plus d’une considération du contexte mathématique."
    en: "A blocking state is a cognitive state in which a student cannot make any progress toward finding a solution to a problem. In this research, we present the development of probabilistic models to detect a blocking state while solving a Canadian high school-level problem in Euclidean geometry on an intelligent tutoring system. Our methodology includes an experimentation with a modified version of QED-Tutrix, an intelligent tutoring system, which was used to gather labelled datasets composed of sequences of mouse and keyboard actions. We observed blocking states in this dataset from subsequence distributions and frequency of states. Using a probabilistic framework, we developed four predicting models: an actionfrequency model, a subsequence-detection model, a 1D convolutional neural network model and an hybrid model. The hybrid model outperforms the others with a F1 score of 80.4 % on classification of blocking state on training set. It performs 77.3 % on test set. The applications of this research lead to an upgrade of QED-Tutrix internal finite-state machine for its interactions with the learner. Also, this research opens a second research stage, in which targeted tutorial interventions in QED-Tutrix can be developed. This can be achieved with an algorithm that understands the context of intervention and that is able to help precisely the learner. In order to get better performances from the current models, the history of the previous blocking states needs to be incorporated. Moreover, the mathematical concepts used by the learner can be integrated."
  thesisUrl: "https://publications.polymtl.ca/3279"
  publications:
    - stlouis2016

- _id: fmartelmaster
  type: master
  student: Capucine Mailloux
  supervisor: Alphonsine Beauchamnps
  current: true
  title:
    fr: "Amélioration des modèles de représentation des graphes de connaissance par le plongement de règles logiques "
    en: "Improving knowledge graph embeddings by learning vector representations of logical rules"
  description:
    fr: |
      La plupart des modèles de plongement pour les graphes de connaissance, comme TransE et ses dérivés, s'appuient uniquement sur les instances (entités et relations) contenues dans le graphe et laissent de côté les ontologies et axiomes qui définissent la structure même de ces graphes. Dans ce travail, on cherche au contraire à intégrer les axiomes issus de la logique descriptive dans ces modèles de plongement. En particulier, il s'agit de construire un modèle d'embedding capable de construire des représentations vectorielles à la fois pour les entités et pour les axiomes logiques, de telle sorte que les propriétés logiques entre éléments se traduisent par des propriétés géométriques de leurs embeddings.
    en: |
      Knowledge graph embeddings provide an efficient and convenient way to handle large knowledge graphs and to get dense, meaningful representations of their entities. Yet, most of these embedding models are trained using only the entities contained in the graph and thus don't take advantage of the underlying ontology. In this work, I try to incorporate logical axioms into embedding models. One of the goal here is to find a model that can compute vector representations for both entities and logical rules, such that the geometric properties of the embeddings reflect the logical relationships between the elements.

- _id: lclouatremaster
  type: master
  student: Christophe Rodrigue
  supervisor: Alphonsine Beauchamnps
  current: true
  title:
    fr: Question et réponse automatique avec réseau de neuronnes profonds
    en: Automatic question and answering with deep neural networks
  description:
    fr: |
      Avec l'aide d'apprentissage profond, nous créons des modèles capables de réponses automatiques a des question sur des connaissances en format de language naturel.
    en: |
      With the help of deep learning, we create models capable of automatic question answering with regards to knowledge in natural language format.

- _id: hghavidelphd
  type: phd
  student: Clothide Monty
  supervisor: Alphonsine Beauchamnps
  cosupervisor: Mohammed Fellag
  current: true
  title:
    fr: "Classement Automatique des Réponses Courtes"
    en: "Automatic Short Answer Grading"
  description:
    fr: |
      L'un des derniers développements dans le domaine de l'éducation a été l'émergence de cours ouverts en ligne. Ce type d'apprentissage permet à un nombre illimité d'élèves du monde entier d'accéder librement à l'enseignement.  Les fournisseurs de ce type de cours demandent aux éducateurs de réformer les approches traditionnelles dans l'évaluation de la compréhension des élèves et la fourniture de la rétroaction. Cela implique le développement d'outils automatisés pour représenter les connaissances dans le contenu du cours, le suivi de la compréhension globale des concepts du cours et des progrès en classe, l'évaluation des connaissances disciplinaires des étudiants et l'évaluation automatique des réponses libres aux questions.
      Dans ce contexte éducatif, ce projet se concentre sur le développement de méthodes d'IA pour évaluer automatiquement les réponses courtes des élèves. Dans un sens plus large, cette recherche aidera à faire progresser l'évaluation de la compréhension des connaissances fondée sur l'IA.
    en: |
      One of the latest developments in the field of education has been the emergence of Massive open online courses (MOOCs). This type of learning provides open access education for an unlimited number of students around the world.  MOOCs providers require educators to reform the traditional approaches in the assessment of student understanding and provision of the feedback. This involves the development of automated tools for representing the knowledge in course content, tracking the overall comprehension of the course concepts and class progress, assessing students’ disciplinary literacy and automatically evaluating free responses to questions.
      In this educational context, this project focuses on the development of AI methods for automatically evaluating the students’ short answers. In a broader sense, this research will help advance the AI-based evaluation of human knowledge.

- _id: jcorbeilphd
  type: phd
  student: Robert de Chateaub
  supervisor: Alphonsine Beauchamnps
  cosupervisor: Michel Deslauriers
  current: true
  title:
    fr: "Analyse de la littératie disciplinaire avec les méthodes du traitement de la langue naturelle et d’apprentissage profond"
    en: "Analysis of Disciplinary Literacy with Natural Language Processing and Deep Learning Methods"
  description:
    en: |
      Disciplinary Literacy is defined in the educational literature as the mastery of a discipline language and ways of thinking. The best way to learn a discipline or to assess someone expertise in a discipline is by analyzing is disciplinary literacy. Yet, in natural language processing, it is still not clear how to define it. What are the features of disciplinary literacy in natural language processing?  Can we use recent methods from deep learning like attention mechanisms and language models to represent it? These are the questions that I am looking to answer during my doctorate studies.
    fr: |
      La littératie disciplinaire est définie dans la littérature comme la maîtrise du langage et des façons de réfléchir dans un domaine d'expertise. La meilleure façon d'apprendre ou de quantifier l'expertise de quelqu'un dans une  discipline passe par l'analyse de sa littératie disciplinaire. Cependant, en traitement de la langue naturelle, il n'est pas encore claire comment exprimer le concept de littératie disciplinaire. Quels sont les caractéristiques de la littératie disciplinaire en traitement de la langue naturelle? Est-il possible d'utiliser des techniques avancées comme les mécanismes d'attention et les modèles de langage pour la représenter? Voilà les questions que je cherche à résoudre dans le cadre de mon doctorat.

- _id: ldenigerphd
  type: phd
  student: Christophe Deniger
  supervisor: Michel Deslauriers
  cosupervisor: Alphonsine Beauchamnps
  current: true
  title:
    fr: Création d'un générateur de preuves à des problèmes de géométrie du secondaire simulant le modèle de pensée d'un élève, pour le logiciel tuteur QED-Tutrix.
    en: Creation of a proof generator for high school geometry problems that simulates the resolution process of a student, for the tutor software QED-Tutrix.
  description:
    fr: |
      Le logiciel tuteur QED-Tutrix offre une interface permettant à un élève de secondaire de résoudre des problèmes de géométrie et de l'accompagner tout au long de cette résolution. Cet accompagnement nécessite d'être capable d'anticiper les différentes preuves accessibles à l'élève, et donc une connaissance a priori de l'ensemble des preuves possibles. L'encodage manuel de cette connaissance par un expert étant particulièrement long et pénible, nous avons pour objectif d'automatiser au maximum ce travail en implémentant un générateur de preuves capable de construire automatiquement toutes les preuves à un problème qu'un élève pourrait écrire.
    en: |
      The tutor software QED-Tutrix provides to high school students an interface to solve geometry proof problems, including a tutoring system to help them during the whole resolution process. This tutoring requires the anticipation of the various proofs the student could write, and therefore an a priori knowledge of all the possible proofs for a given problem. The manual encoding by an expert of this knowledge is particularly tedious and time-consuming. We therefore have the goal to automate this process as much as possible, by implementing an automated proof generator that generates all the proof a student could have written.

- _id: emarchandmaster
  type: master
  student: Damien Noël
  supervisor: Michel Deslauriers
  cosupervisor: Alphonsine Beauchamnps
  current: true
  title:
    fr: "Extraction de relations de composition au sein de documents archéologiques"
    en: "Extraction of composition relations within archaeological documents"
  description:
    fr: |
      Le Ministère de la Culture et des Communications de Québec possède de nombreux rapports archéologiques. Ces documents contiennent de nombreuses informations intéressantes, pour des cadres éducatifs par exemple, mais celles-ci sont perdus dans ces rapports non utilisés. L'objectif du projet de recherche est donc de développer un outil capable, dans un premier temps, de localiser les entités intéressantes au sein du texte puis d'identifier les relations les liants. Dans le cadre de ce projet, seul la relation de composition entre artefacts et matériaux, très présente dans ces textes, sera considérée. L'objectif du travail est donc, à l'aide du traitement de langue naturelle et de l'extraction d'entités et relations, de pouvoir détecter au sein de texte la présence de relations de compositions entre artefacts et matériaux. Le but de ce travail est, par la suite, de pouvoir peupler une ontologie existante pour permettre l'exploitation plus aisée de ces informations.
    en: |
      The Quebec Ministry of Culture and Communications has many archaeological reports. These documents contain a lot of interesting information, for educational purposes for example, but these are lost in these unused reports. The objective of the research project is therefore to develop a tool capable, at first, of locating the interesting entities within the text and then identifying the relationships between them. As part of this project, only the composition relationship between artifacts and materials, very present in these texts, will be considered. The objective of the work is, using the natural language processing and the extraction of entities and relations, to be able to detect within text the presence of relations of compositions between artifacts and materials. The purpose of this work is, afterwards, to be able to populate an existing ontology to allow the easier exploitation of this informations.
